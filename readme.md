# RAG Evaluation Framework

This project implements various RAG (Retrieval-Augmented Generation) evaluation frameworks to assess and compare different RAG implementations using the RAGAS methodology.

## ğŸš€ Features

- Multiple RAG implementation support:
  - âœ… LlamaIndex
  - ğŸ”„ LangChain (in progress)
  - ğŸ”„ Haystack (in progress)
- Standardized evaluation metrics
- Data indexing capabilities
- Comparative analysis tools

## ğŸ“‹ Prerequisites

- Python 3.8+
- Required packages (see `requirements.txt`)
- Data files for indexing

## ğŸ› ï¸ Installation

1. Clone the repository:
```bash
git clone [your-repo-url]
cd rag-eval-react
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

## ğŸ“– Usage

1. Data Indexing:
   - Place your data in the `data/` directory
   - Run the indexing script for your chosen implementation

2. Running Evaluations:
```bash
python -m llamaindex.llamaindex_ragas
```

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ data/                  # Data directory for storing documents
â”‚   â””â”€â”€ gold_data.json    # Gold standard evaluation data
â”œâ”€â”€ llamaindex/           # LlamaIndex implementation
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ llamaindex_ragas.py
â”œâ”€â”€ requirements.txt      # Project dependencies
â””â”€â”€ README.md            # Project documentation
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.
